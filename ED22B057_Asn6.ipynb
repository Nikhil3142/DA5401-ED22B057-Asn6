{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad65dd9",
   "metadata": {},
   "source": [
    "##  Part A — Data Preprocessing and Imputation\n",
    "\n",
    "### 1 Load and Prepare the Dataset\n",
    "We begin by loading the UCI Credit Card Default Clients Dataset and cleaning the column names for ease of access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791dd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/nikhil.narayan/Documents/GOOGLE DRIVE/SEM_7/DA5401/ASN6/UCI_Credit_Card.csv\")\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad2545",
   "metadata": {},
   "source": [
    "###  Introduce Artificial Missing Values (MAR)\n",
    "\n",
    "We simulate a *Missing At Random (MAR)* scenario by randomly removing 5–10 % of values from a few numerical columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34886307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE          8.0\n",
       "BILL_AMT1    8.0\n",
       "BILL_AMT2    8.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_missing = ['AGE', 'BILL_AMT1', 'BILL_AMT2']\n",
    "missing_fraction = 0.08\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "for col in cols_with_missing:\n",
    "    n_missing = int(missing_fraction * len(df))\n",
    "    idx = rng.choice(df.index, n_missing, replace=False)\n",
    "    df.loc[idx, col] = np.nan\n",
    "\n",
    "df[cols_with_missing].isna().mean() * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdbf7e",
   "metadata": {},
   "source": [
    "###  2 Imputation Strategy 1 — Simple (Median) Imputation → Dataset A\n",
    "\n",
    "We replace missing values in each column by the column median to get Dataset A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8690847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rr/zvwk87k103j5rkj9rb333nfh0000gp/T/ipykernel_88740/1003143582.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_A[col].fillna(df_A[col].median(), inplace=True)\n",
      "/var/folders/rr/zvwk87k103j5rkj9rb333nfh0000gp/T/ipykernel_88740/1003143582.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_A[col].fillna(df_A[col].median(), inplace=True)\n",
      "/var/folders/rr/zvwk87k103j5rkj9rb333nfh0000gp/T/ipykernel_88740/1003143582.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_A[col].fillna(df_A[col].median(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AGE          0\n",
       "BILL_AMT1    0\n",
       "BILL_AMT2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A = df.copy()\n",
    "for col in cols_with_missing:\n",
    "    df_A[col].fillna(df_A[col].median(), inplace=True)\n",
    "df_A[cols_with_missing].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328dd1b2",
   "metadata": {},
   "source": [
    "**Why median over mean?**\n",
    "\n",
    "Median is **robust to outliers**.  \n",
    "Financial data (like `BILL_AMT`) are often skewed; extreme values can distort the mean.  \n",
    "The median represents the typical case more accurately, producing a stable central tendency for imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62bf78",
   "metadata": {},
   "source": [
    "###  3 Imputation Strategy 2 — Linear Regression Imputation → Dataset B\n",
    "\n",
    "Use a Linear Regression model to predict the missing values in one column (e.g., `BILL_AMT1`) from all other features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac70057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_B = df.copy()\n",
    "target_col = 'BILL_AMT1'\n",
    "\n",
    "train_rows = df_B[df_B[target_col].notna()]\n",
    "test_rows  = df_B[df_B[target_col].isna()]\n",
    "\n",
    "predictors = [c for c in df_B.columns if c not in ['ID', target_col, 'default_payment_next_month']]\n",
    "\n",
    "X_train = train_rows[predictors].fillna(train_rows.median())\n",
    "y_train = train_rows[target_col]\n",
    "X_pred  = test_rows[predictors].fillna(train_rows.median())\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "df_B.loc[df_B[target_col].isna(), target_col] = lin_reg.predict(X_pred)\n",
    "df_B[target_col].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf885e",
   "metadata": {},
   "source": [
    "**Explanation — Linear Regression Imputation**\n",
    "\n",
    "This approach assumes the data are *Missing At Random (MAR)*, meaning missingness depends on observed features (e.g., AGE or LIMIT_BAL) but not on the missing values themselves.  \n",
    "Linear relationships among predictors allow us to estimate the most plausible values for `BILL_AMT1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e219298",
   "metadata": {},
   "source": [
    "###  4 Imputation Strategy 3 — Non-Linear Regression (KNN / Decision Tree) → Dataset C\n",
    "\n",
    "We now apply a non-linear method that can capture complex relationships between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aeb0d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_C = df.copy()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "num_df = df_C.select_dtypes(include=np.number)\n",
    "df_C_imputed = pd.DataFrame(imputer.fit_transform(num_df), columns=num_df.columns)\n",
    "\n",
    "for col in df_C_imputed.columns:\n",
    "    df_C[col] = df_C_imputed[col]\n",
    "\n",
    "df_C[target_col].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63aca2a",
   "metadata": {},
   "source": [
    "**Explanation — Non-Linear Regression Imputation**\n",
    "\n",
    "K-Nearest Neighbors imputation estimates a missing value from the average of its closest samples in the feature space.  \n",
    "This method captures non-linear relationships that linear models might miss.  \n",
    "Alternatives like Decision Tree Regressors can also approximate non-linear patterns via if–else splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae355b",
   "metadata": {},
   "source": [
    "###  Summary of Part A\n",
    "\n",
    "| Dataset | Imputation Method | Key Idea | Assumption | Expected Bias |\n",
    "|:--:|:--|:--|:--|:--|\n",
    "| A | Median (Simple) | Replace NaN with column median | MCAR / mild MAR | Low |\n",
    "| B | Linear Regression | Predict missing values linearly | MAR + linear relations | Moderate |\n",
    "| C | Non-Linear (KNN/Tree) | Predict non-linearly from neighbors | MAR + non-linear relations | Low–Moderate |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cb18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d591e47",
   "metadata": {},
   "source": [
    "## Part B — Model Training and Performance Assessment\n",
    "\n",
    "We now evaluate how each imputation strategy affects downstream model accuracy.\n",
    "We’ll train a Logistic Regression classifier on four datasets:\n",
    "A (Median Imputation), B (Linear Regression Imputation), C (Non-Linear Imputation), and D (Listwise Deletion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d945823",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dfX in zip(['B','C'], [df_B, df_C]):\n",
    "    dfX.fillna(dfX.median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b75df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695fb94",
   "metadata": {},
   "source": [
    "### 1 – Data Split\n",
    "For each imputed dataset, we split the data into training and testing sets (80 / 20).\n",
    "Dataset D is created by dropping rows containing any missing values before the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a066cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'default.payment.next.month'\n",
    "\n",
    "# Helper to split data\n",
    "def split_data(df):\n",
    "    X = df.drop(columns=[target, 'ID'])\n",
    "    y = df[target]\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Datasets A–C (already imputed)\n",
    "splits = {}\n",
    "for name, data in zip(['A','B','C'], [df_A, df_B, df_C]):\n",
    "    splits[name] = split_data(data)\n",
    "\n",
    "# Dataset D – Listwise Deletion\n",
    "df_D = df.dropna()\n",
    "splits['D'] = split_data(df_D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5a5aa",
   "metadata": {},
   "source": [
    "### 2 – Standardization\n",
    "We standardize features in every dataset using `StandardScaler`\n",
    "so that the Logistic Regression model is not biased by differing feature scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1d8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_splits = {}\n",
    "\n",
    "for name, (X_train, X_test, y_train, y_test) in splits.items():\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    scaled_splits[name] = (X_train_scaled, X_test_scaled, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee798d",
   "metadata": {},
   "source": [
    "### 3 – Model Training and Evaluation\n",
    "We train a Logistic Regression classifier on each dataset\n",
    "and evaluate it using Accuracy, Precision, Recall, and F1-Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868325cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset A Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.817     0.969     0.887      4673\n",
      "           1      0.684     0.238     0.353      1327\n",
      "\n",
      "    accuracy                          0.807      6000\n",
      "   macro avg      0.751     0.603     0.620      6000\n",
      "weighted avg      0.788     0.807     0.769      6000\n",
      "\n",
      "\n",
      "=== Dataset B Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.969     0.887      4673\n",
      "           1      0.690     0.243     0.359      1327\n",
      "\n",
      "    accuracy                          0.808      6000\n",
      "   macro avg      0.754     0.606     0.623      6000\n",
      "weighted avg      0.790     0.808     0.770      6000\n",
      "\n",
      "\n",
      "=== Dataset C Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.818     0.969     0.887      4673\n",
      "         1.0      0.688     0.243     0.359      1327\n",
      "\n",
      "    accuracy                          0.808      6000\n",
      "   macro avg      0.753     0.606     0.623      6000\n",
      "weighted avg      0.790     0.808     0.770      6000\n",
      "\n",
      "\n",
      "=== Dataset D Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.815     0.977     0.889      3625\n",
      "           1      0.746     0.235     0.358      1050\n",
      "\n",
      "    accuracy                          0.810      4675\n",
      "   macro avg      0.781     0.606     0.623      4675\n",
      "weighted avg      0.800     0.810     0.769      4675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, (X_train, X_test, y_train, y_test) in scaled_splits.items():\n",
    "    clf = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"\\n=== Dataset {name} Results ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8903e5",
   "metadata": {},
   "source": [
    "## Part C — Comparative Analysis\n",
    "\n",
    "### 1 – Results Comparison\n",
    "\n",
    "The following table summarizes performance across all four datasets.\n",
    "Along with Accuracy and F1-score, we include **precision for class 0 (non-defaulters)** and **class 1 (defaulters)** to highlight class imbalance.\n",
    "\n",
    "| Model | Imputation Method | Accuracy | Precision (0) | Precision (1) | F1-Score (1) | Key Comments |\n",
    "|:--:|:--|:--:|:--:|:--:|:--:|:--|\n",
    "| **A** | Median (Simple) | 0.807 | 0.817 | 0.684 | 0.353 | Baseline; robust and easy to apply. |\n",
    "| **B** | Linear Regression | 0.808 | 0.818 | 0.690 | 0.359 | Slightly higher precision/F1; fits linear MAR assumption. |\n",
    "| **C** | Non-Linear (KNN) | 0.808 | 0.818 | 0.688 | 0.359 | Same as B → non-linear effects minimal. |\n",
    "| **D** | Listwise Deletion | 0.810 | 0.815 | 0.746 | 0.358 | Comparable accuracy; fewer training rows. |\n",
    "\n",
    "**Interpretation:**  \n",
    "All models achieve similar overall accuracy (~80 %), but regression-based imputations slightly improve precision and F1 for the minority class (defaults).  \n",
    "This shows that even a limited imputation strategy can recover small but meaningful predictive information.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 – Why Accuracy Can Be Misleading\n",
    "\n",
    "In credit-default prediction, the dataset is **imbalanced** — most customers are non-defaulters.  \n",
    "A classifier that always predicts “no default” would still reach ~80 % accuracy, yet completely fail to identify actual defaulters.  \n",
    "Therefore, **Accuracy overstates performance** on such data; metrics like **Precision, Recall, and F1-score for class 1** are much better indicators of true effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 – Efficacy Discussion\n",
    "\n",
    "**Listwise Deletion (D):**  \n",
    "- Removes every record containing any NaN, reducing sample size and potentially altering the data distribution.  \n",
    "- Its accuracy appears slightly higher but this can be an artifact of easier majority-class prediction.  \n",
    "- Loss of minority examples weakens generalization.\n",
    "\n",
    "**Imputation Models (A–C):**  \n",
    "- Preserve data volume and representativeness.  \n",
    "- Model A (Median) works best for MCAR data.  \n",
    "- Model B (Linear) captures relationships under MAR, giving a small gain in precision and F1.  \n",
    "- Model C (Non-Linear KNN) adds flexibility but yields similar results here—implying that the underlying relationship between `BILL_AMT1` and other predictors is mostly linear.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 – Conclusion and Recommendation\n",
    "\n",
    "- **Best Practical Strategy:** Linear-regression imputation (Model B).  \n",
    "  It provides a slight but consistent lift in minority-class metrics with minimal extra complexity.  \n",
    "- **When to Use Median:** Quick baseline or nearly complete data.  \n",
    "- **When to Use Listwise Deletion:** Only if missingness < 2 % or interpretability outweighs potential bias.\n",
    "\n",
    "Overall, regression-based imputation preserves information and stability, while accuracy alone should never be used to judge performance in imbalanced classification problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
